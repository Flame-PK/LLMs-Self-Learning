{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM 就是大语言模型 Large Language Model\n",
    "\n",
    "生成模型有三个阶段 预训练preTrain，监督微调SFT，强化学习RLHF\n",
    "\n",
    "工作流程：io流 当我们有对应输入，会有怎样的输出\n",
    "1Tokenization 分词化\n",
    "    将句子分解成更小的unit，有不同的颗粒分类，不同的语言遵循不同的方式\n",
    "        词粒度Word_Level Tokenization 适用于英语类似的语言\n",
    "        字符粒度Character-Level 适用于中文类似的语言\n",
    "        子词粒度Subword_level 适用于有词缀词根相关的语言、\n",
    "工作方式\n",
    "    通过给定的token预测下一个token 按照以下recursive方式去进行预测 \n",
    "        1. 基于现有tokens 预测下一个最有可能的token\n",
    "        2. 将得到的token加入到现有的tokens里\n",
    "        3. 重复以上操作\n",
    "        4. 直到输出特殊token 如句号，end of sentence，换行符号之类的\n",
    "\n",
    "完成数据收集和token分词后，进入到模型构建部分 主要运用到的是Transformer架构\n",
    "\n",
    "Transformer块结构：\n",
    "    Input → [LayerNorm] → [Multi-Head Attention] → [Residual Add] → [LayerNorm] → [Feedforward] → [Residual Add] → Output\n",
    "\n",
    "    1. 输入嵌入 + 位置编码\n",
    "    输入嵌入就是将Token ID转换为向量\n",
    "    位置编码则是注入顺序信息\n",
    "\n",
    "    细节： 将word变化为vector的过程 叫做word2vector 每个词会按照预设的值转变为vector（这个初始值是随机的 但你要知道 你自己手动怕不是得累死 因此大伙都用之前有好人已经提供的matrix作为初始值）比如chatgpt3 它的初始word vector就有大概50257个词*12288个dimension\n",
    "\n",
    "    而训练，就是将语义相近的词的vector尽可能的靠拢（在空间上） 因此如果直接使用训练好的word matrix，也许仅需再训练一点就能直接达成目的？\n",
    "\n",
    "    一个简短的例子 假设function E()是word转化为vector 那么\n",
    "    E(queen) - E(king) ≈ E(woman) - E(man) \n",
    "\n",
    "    随后是点积 dot product 这是注意力机制的根本，我们使用它作为衡量word vector的对齐程度的方法 （我认为可以理解成两个词中间的特殊关系）\n",
    "    向量方向相近 dot product为正\n",
    "    向量相互垂直 dot product为零\n",
    "    向量方向相反 dot product为负\n",
    "    举个例子 \n",
    "    代表负数的plur可以通过E(cats) - E(cat)获得\n",
    "    假设plur * E(cat) 那么我们就可以得到负数 因为cat是单数\n",
    "    plur * E(cats) 那么我们就可以得到负数 因为cats是复数\n",
    "    同样的，当我们运用在dog和dogs上，理应得到类似的结果\n",
    "    有趣的是，如果我们将plur * E(one) = -2.4\n",
    "    随后尝试其他数字     \n",
    "    plur * E(Two) = 0.79\n",
    "    plur * E(Three) = 1.27\n",
    "    plur * E(Four) = 1.80\n",
    "    可以看到他们是在增长的\n",
    "\n",
    "    以上 就是初始的Embedding Matrix，总共有50257*12288个参数 我们把它叫做W_e\n",
    "\n",
    "    但当我们将一大篇文本直接丢进去的时候，我们仅仅是将每个单词单个的从embedding matrix中翻出来罢了，他们之间没有上下文，没有情景，也没有位置关系，我们要做的，就是更新vector并使他们能够获得比单个词更丰富更具体的含义 \n",
    "\n",
    "    但我们在处理过程中 我们的网络仅能一次处理特定数量的向量 GPT3的context size就是2048，这个大小限制了GPT的上下文长度，因此超出这个长度时，我们会发现它变得健忘\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Pseudocode\n",
    "    input_embedding = embedding_layer(token_ids)  # shape: [seq_len, d_model]\n",
    "    position_embedding = get_position_encoding(seq_len, d_model)\n",
    "    x = input_embedding + position_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. 多头自注意力机制\n",
    "    Q = Query\n",
    "    K = Key\n",
    "    V = Value\n",
    "    根据这三个进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Pseudocode\n",
    "    Q = x @ W_q   # shape: [seq_len, d_k]\n",
    "    K = x @ W_k\n",
    "    V = x @ W_v\n",
    "    attention_weights = softmax(Q @ K.T / sqrt(d_k))\n",
    "    output = attention_weights @ V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Add & LayerNorm\n",
    "    将原始输入添加到注意力输出中 也就是残差连接residual connection\n",
    "    规范化结果以稳定学习过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    x = LayerNorm(x + Attention(Q, K, V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. 前馈网络FFN (MLP aka multiple layer perceptron 多层感知器) 两个名字都可以 取决于input和处理过程\n",
    "    \n",
    "    在attention之后，我们将一个简单的MLP独立的应用于每个标记的位置\n",
    "    FFN(x) = ReLU(xW1 + b1)W2 + b2\n",
    "    This helps model complex combinations of features per token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5. Final Residual + LayerNorm\n",
    "    与之前相同 添加输入 然后规范化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    x = LayerNorm(x + FFN(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    以上就是完整的一次transformer流程，通常我们将这个过程堆积12/24/96次 每一次都利用更深层次的上下文来细化表征\n",
    "    在最后一次迭代后 最终output会被传递到线性层 -> softmax over vocab\n",
    "    用于在训练期间预测下一个标记"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    UnEmbedding\n",
    "    最后阶段，我们需要输出下一个可能的token的分布概率。\n",
    "    举个例子，假如我们让这个网络了解了哈利波特这本书，并且在我们的input中包含了关键词“哈利波特”“最讨厌的”“教授” \n",
    "    那么按照我们context matrix最后一数列的那1*12288的vector，我们将它和我们所有的 12288*50257大小的矩阵W_u 也就是UnEmbedding Matrix进行映射，会得到一个1*50257大小的概率表 之后对表里每一个做softmax 最终得到每个单词的概率表 一般来说，概率最高的是概率表里数字最大的word 这个词大概率就是Snape\n",
    "\n",
    "    为什么只选择最后一列呢？因为最后一列已经是经过前面的选择而得出的，它本身就蕴含着前面的words的信息，因此为了效率最大化，我们只选择最后一列\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
